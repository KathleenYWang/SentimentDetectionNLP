{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We create vocab for the models by combining all the train dataset.\n",
    "Since we work with multiple datasets and building vocab based on them, it makes sense to build a combined vocab to be used for all training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 1grams ...\n",
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 2grams ...\n",
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.early_stopping import EarlyStopping\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from model.ha import HierarchicalAttPredictor\n",
    "from sklearn.metrics import classification_report\n",
    "# from data.evaluate import load_dev_labels, get_metrics\n",
    "import pickle as pkl\n",
    "import sys\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import random\n",
    "from utils.focalloss import FocalLoss\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "from utils.tweet_processor import processing_pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from module.preprocessor import EnglishPreProcessor\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from config.basic_config import configs as config\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import create_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc pipeline not working, have to remove it\n",
    "def load_data_context(data_path='/data/SuperMod/test_data.txt', is_train=True):\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    if len(df.columns) > 4:\n",
    "        data_list = df.comment_text.tolist()\n",
    "        target_list = df.toxic.tolist()   \n",
    "\n",
    "    else:\n",
    "        data_list = df.comment_text.tolist()\n",
    "        target_list = df.toxicity.tolist()\n",
    "\n",
    "    clean_sent_list = [sent_tokenize(processing_pipeline(str(email))) for email in data_list]\n",
    "\n",
    "    if is_train:\n",
    "        return clean_sent_list, target_list\n",
    "    else:\n",
    "        return clean_sent_list\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train = pd.read_csv('/data/SuperMod/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test_with_label instead of the actual test because not all test has label\n",
    "wiki_test = pd.read_csv('/data/SuperMod/test_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s, label_s = load_data_context(data_path='/data/SuperMod/training_data_supermod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t, label_t = load_data_context(data_path='/data/SuperMod/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v, label_v = load_data_context(data_path='/data/SuperMod/test_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mt, label_mt = load_data_context(data_path=\"/data/SuperMod/imdb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m, label_m = load_data_context(data_path=\"/data/SuperMod/imdb_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 24205 words\n",
      "filling vocab to 20003\n"
     ]
    }
   ],
   "source": [
    "word2id, id2word, num_of_vocab = create_data.build_vocab([train_s, train_t, train_v, train_mt, train_m], 20000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/SuperMod/word2id.pkl', 'wb') as w:\n",
    "    pkl.dump(word2id, w, pkl.HIGHEST_PROTOCOL)\n",
    "with open('/data/SuperMod/id2word.pkl', 'wb') as i:\n",
    "    pkl.dump(id2word, i, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_mini = wiki_train[:100]\n",
    "wiki_mini.to_csv('/data/SuperMod/train_mini.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test out train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 1grams ...\n",
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 2grams ...\n",
      " stats_dir /home/kathleenwang/.ekphrasis/stats\n",
      "Reading twitter - 1grams ...\n",
      "Tokenizing using dictionary from /data/torchMoji/model/vocabulary.json\n",
      "creating pkl file for the emb text file\n",
      "Loading Glove Model\n",
      "100%|██████████████████████████████| 2196017/2196017 [02:21<00:00, 15480.68it/s]\n",
      "Done. 2196016  words loaded!\n",
      "loading glove\n",
      "100%|█████████████████████████████████| 20003/20003 [00:00<00:00, 377177.74it/s]\n",
      "15614 of 20003 found coverage 0.7805829125631155\n",
      "Size of final test data 100\n",
      "Train size: 50 Dev size: 50\n",
      "Training on fold: 0\n",
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n",
      "Begin training epoch: 0...\tCurrent learning rate [0.0005]\n",
      "  0%|                                                  | 0/0.25 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/336 [00:00<?, ?B/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 336/336 [00:00<00:00, 1396715.70B/s]\u001b[A\n",
      "  0%|                                              | 0/374434792 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|                          | 1462272/374434792 [00:00<00:26, 14230292.32B/s]\u001b[A\n",
      "  3%|▋                         | 9361408/374434792 [00:00<00:19, 18871835.41B/s]\u001b[A\n",
      "  5%|█▏                       | 17761280/374434792 [00:00<00:14, 24591820.38B/s]\u001b[A\n",
      "  7%|█▋                       | 25733120/374434792 [00:00<00:11, 31028805.47B/s]\u001b[A\n",
      "  9%|██▎                      | 33854464/374434792 [00:00<00:08, 38089841.22B/s]\u001b[A\n",
      " 11%|██▋                      | 40754176/374434792 [00:00<00:07, 44002723.40B/s]\u001b[A\n",
      " 13%|███▏                     | 47206400/374434792 [00:00<00:06, 48443098.78B/s]\u001b[A\n",
      " 14%|███▌                     | 53623808/374434792 [00:00<00:06, 51730428.26B/s]\u001b[A\n",
      " 16%|████                     | 59960320/374434792 [00:00<00:05, 53485983.92B/s]\u001b[A\n",
      " 18%|████▍                    | 67095552/374434792 [00:01<00:05, 55642164.87B/s]\u001b[A\n",
      " 20%|█████                    | 75205632/374434792 [00:01<00:04, 61426906.90B/s]\u001b[A\n",
      " 22%|█████▍                   | 81955840/374434792 [00:01<00:04, 61214756.34B/s]\u001b[A\n",
      " 24%|██████                   | 90138624/374434792 [00:01<00:04, 65653641.89B/s]\u001b[A\n",
      " 26%|██████▌                  | 98515968/374434792 [00:01<00:03, 70208060.13B/s]\u001b[A\n",
      " 28%|██████▊                 | 105896960/374434792 [00:01<00:03, 69575478.27B/s]\u001b[A\n",
      " 30%|███████▏                | 113107968/374434792 [00:01<00:03, 67714883.17B/s]\u001b[A\n",
      " 32%|███████▋                | 120068096/374434792 [00:01<00:04, 63012149.35B/s]\u001b[A\n",
      " 34%|████████                | 126567424/374434792 [00:01<00:04, 54217096.22B/s]\u001b[A\n",
      " 36%|████████▌               | 134246400/374434792 [00:02<00:04, 59460297.07B/s]\u001b[A\n",
      " 38%|█████████               | 141992960/374434792 [00:02<00:03, 63916242.99B/s]\u001b[A\n",
      " 40%|█████████▌              | 149544960/374434792 [00:02<00:03, 67002241.84B/s]\u001b[A\n",
      " 42%|██████████              | 157282304/374434792 [00:02<00:03, 69800985.31B/s]\u001b[A\n",
      " 44%|██████████▌             | 164919296/374434792 [00:02<00:02, 71648300.17B/s]\u001b[A\n",
      " 46%|███████████             | 172587008/374434792 [00:02<00:02, 73085817.20B/s]\u001b[A\n",
      " 48%|███████████▌            | 180692992/374434792 [00:02<00:02, 75308256.38B/s]\u001b[A\n",
      " 50%|████████████            | 188620800/374434792 [00:02<00:02, 76456209.61B/s]\u001b[A\n",
      " 52%|████████████▌           | 196404224/374434792 [00:02<00:02, 76860526.42B/s]\u001b[A\n",
      " 55%|█████████████           | 204326912/374434792 [00:02<00:02, 77547333.15B/s]\u001b[A\n",
      " 57%|█████████████▌          | 212443136/374434792 [00:03<00:02, 78596795.80B/s]\u001b[A\n",
      " 59%|██████████████          | 220336128/374434792 [00:03<00:01, 77130472.21B/s]\u001b[A\n",
      " 61%|██████████████▌         | 228123648/374434792 [00:03<00:01, 77351972.98B/s]\u001b[A\n",
      " 63%|███████████████▏        | 236499968/374434792 [00:03<00:01, 79169034.24B/s]\u001b[A\n",
      " 65%|███████████████▋        | 244442112/374434792 [00:03<00:02, 62192905.61B/s]\u001b[A\n",
      " 67%|████████████████▏       | 251649024/374434792 [00:03<00:01, 63970353.54B/s]\u001b[A\n",
      " 69%|████████████████▋       | 259885056/374434792 [00:03<00:01, 68560732.02B/s]\u001b[A\n",
      " 71%|█████████████████       | 267128832/374434792 [00:03<00:01, 66600874.67B/s]\u001b[A\n",
      " 73%|█████████████████▌      | 274072576/374434792 [00:04<00:01, 61207520.93B/s]\u001b[A\n",
      " 75%|██████████████████      | 281689088/374434792 [00:04<00:01, 65038027.60B/s]\u001b[A\n",
      " 77%|██████████████████▌     | 289084416/374434792 [00:04<00:01, 67477396.44B/s]\u001b[A\n",
      " 79%|███████████████████     | 296951808/374434792 [00:04<00:01, 70485109.10B/s]\u001b[A\n",
      " 81%|███████████████████▌    | 304709632/374434792 [00:04<00:00, 72472452.39B/s]\u001b[A\n",
      " 83%|████████████████████    | 312097792/374434792 [00:04<00:00, 71761714.15B/s]\u001b[A\n",
      " 85%|████████████████████▍   | 319374336/374434792 [00:04<00:01, 44519621.80B/s]\u001b[A\n",
      " 87%|████████████████████▉   | 327141376/374434792 [00:04<00:00, 50840582.21B/s]\u001b[A\n",
      " 89%|█████████████████████▍  | 334754816/374434792 [00:05<00:00, 56467983.27B/s]\u001b[A\n",
      " 91%|█████████████████████▉  | 341478400/374434792 [00:05<00:00, 58717836.59B/s]\u001b[A\n",
      " 93%|██████████████████████▎ | 348127232/374434792 [00:05<00:00, 47780402.00B/s]\u001b[A\n",
      " 95%|██████████████████████▋ | 354501632/374434792 [00:05<00:00, 51431323.10B/s]\u001b[A\n",
      " 96%|███████████████████████ | 360701952/374434792 [00:05<00:00, 50150247.61B/s]\u001b[A\n",
      " 98%|███████████████████████▋| 368740352/374434792 [00:05<00:00, 56527445.28B/s]\u001b[A\n",
      "1it [01:17, 77.74s/it]                                                          \u001b[A\n",
      "Training loss: 0.7083326578140259\tDev loss: 0.6531563401222229\n",
      "Final test testing...\n",
      "final_pred_list_test 1\n",
      "final_pred_list_test_concat 100\n",
      "(100,)\n",
      "(100,)\n",
      "True Positives per class :  0.0\n",
      "False Positives per class :  0\n",
      "False Negatives per class :  14\n",
      "/home/kathleenwang/projects/supermod/HA/module/evaluate.py:59: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  precision = truePositives / (truePositives + falsePositives)\n",
      "Accuracy : 0.8600, Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "not best model, ignoring ...\n",
      "Begin training epoch: 1...\tCurrent learning rate [2e-05]\n",
      "1it [01:10, 70.95s/it]                                                          \n",
      "Training loss: 0.6609216928482056\tDev loss: 0.6408722400665283\n",
      "Final test testing...\n",
      "final_pred_list_test 1\n",
      "final_pred_list_test_concat 100\n",
      "(100,)\n",
      "(100,)\n",
      "True Positives per class :  0.0\n",
      "False Positives per class :  0\n",
      "False Negatives per class :  14\n",
      "Accuracy : 0.8600, Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "saving best model ...\n",
      "Gold TESTING RESULTS\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "True Positives per class :  0.0\n",
      "False Positives per class :  0\n",
      "False Negatives per class :  14\n",
      "Accuracy : 0.8600, Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python train_ha.py -folds 2 -epoch 2 -input_path '/data/SuperMod/train_mini.csv' -test_path '/data/SuperMod/train_mini.csv'  -out_path '/data/SuperMod/hapy_state_delete.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
